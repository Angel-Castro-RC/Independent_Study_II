# -*- coding: utf-8 -*-
"""Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nCcZzUmLXtO3mcZLu8O4kmA02FrmwMYp
"""

from transformers import RobertaTokenizer, RobertaForSequenceClassification, BartTokenizer, BartForConditionalGeneration
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load tokenizer and models
# 1. RoBERTa for classification
roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

# 2. LegalBERT for classification (better suited for legal texts)
legalbert_tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')
legalbert_model = AutoModelForSequenceClassification.from_pretrained('nlpaueb/legal-bert-base-uncased', num_labels=2)

# 3. BART for summarization
bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

print("Models and tokenizers loaded successfully!")

# Load the legal text file
with open('legal_document.txt', 'r') as f:
    legal_text = f.read()

print(legal_text[:])

# Summarization using BART
inputs = bart_tokenizer([legal_text], max_length=1024, return_tensors='pt', truncation=True)
summary_ids = bart_model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)

# Decode and print summary
summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print("Summary:", summary)

# Tokenize the text for RoBERTa
inputs = roberta_tokenizer(legal_text, return_tensors="pt", padding=True, truncation=True, max_length=512)

# Perform classification
outputs = roberta_model(**inputs)
logits = outputs.logits
predicted_class = logits.argmax(dim=-1).item()

# Display the result (0 for against, 1 for in favor)
roberta_label = "In Favor of Plaintiff" if predicted_class == 1 else "In Favor of Defendant"
print("RoBERTa Prediction:", roberta_label)

# Tokenize the text for LegalBERT
inputs = legalbert_tokenizer(legal_text, return_tensors="pt", padding=True, truncation=True, max_length=512)

# Perform classification
outputs = legalbert_model(**inputs)
logits = outputs.logits
predicted_class = logits.argmax(dim=-1).item()

# Display the result (0 for against, 1 for in favor)
legalbert_label = "In Favor of Plaintiff" if predicted_class == 1 else "In Favor of Defendant"
print("LegalBERT Prediction:", legalbert_label)

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

# Load DistilBERT for classification
distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

# Tokenize the text for DistilBERT
inputs = distilbert_tokenizer(legal_text, return_tensors="pt", padding=True, truncation=True, max_length=512)

# Perform classification
outputs = distilbert_model(**inputs)
logits = outputs.logits
predicted_class = logits.argmax(dim=-1).item()

# Display the result (0 for against, 1 for in favor)
distilbert_label = "In Favor of Plaintiff" if predicted_class == 1 else "In Favor of Defendant"
print("DistilBERT Prediction:", distilbert_label)

